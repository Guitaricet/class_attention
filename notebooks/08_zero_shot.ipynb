{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to make zero-shot work\n",
    "\n",
    "Multiple model experiments including different normalizations, loss funciton modifications and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import class_attention as cat\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def detorch(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on labels\n",
    "\n",
    "Note that the dataset we use (`Fraser/news-category-dataset`) has some interesting particularities in the class names.\n",
    "\n",
    "For example, it has classes `STYLE` and `STYLE & BEAUTY` or `WORLD NEWS` and `NEWS`. I.e., some classes contain other classes names in their name.\n",
    "The classes that have `&` in their name have a similar particularity. Some of the categories does not seem to be distinguishable. E.g., `THE WORLDPOST` and `WORLDPOST` or `ARTS & CULTURE` and `CULTURE & ARTS`.\n",
    "\n",
    "\n",
    "\n",
    "* &\t: STYLE & BEAUTY, ARTS & CULTURE, HOME & LIVING, FOOD & DRINK, CULTURE & ARTS\n",
    "* VOICES\t: LATINO VOICES, BLACK VOICES, QUEER VOICES\n",
    "* NEWS\t: WEIRD NEWS, GOOD NEWS, WORLD NEWS\n",
    "* ARTS\t: ARTS, ARTS & CULTURE, CULTURE & ARTS\n",
    "* CULTURE\t: ARTS & CULTURE, CULTURE & ARTS\n",
    "* LIVING\t: HEALTHY LIVING, HOME & LIVING\n",
    "* WORLDPOST\t: THE WORLDPOST, WORLDPOST\n",
    "* WORLD\t: THE WORLDPOST, WORLDPOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset news_category (/Users/vladislavlialin/.cache/huggingface/datasets/news_category/default/0.0.0/737b7b6dff469cbba49a6202c9e94f9d39da1fed94e13170cf7ac4b61a75fb9c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving the following classes to a class-test set: ['STYLE & BEAUTY', 'TECH', 'ARTS & CULTURE', 'PARENTS', 'THE WORLDPOST', 'WOMEN', 'FOOD & DRINK', 'WELLNESS']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be35611bf2c43ba9c119f52f60ffbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Dataset:   0%|          | 0/12244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d3510029274a3088604e437e1e0c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Dataset:   0%|          | 0/12244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    all_classes_str,\n",
    "    test_classes_str,\n",
    ") = cat.training_utils.prepare_dataloaders(\n",
    "    test_class_frac=0.2,\n",
    "    batch_size=32,\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    dataset_frac=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAttentionModel(nn.Module):\n",
    "    def __init__(self, txt_encoder, cls_encoder, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.txt_encoder = txt_encoder\n",
    "        self.cls_encoder = cls_encoder\n",
    "\n",
    "        txt_encoder_h = cat.modelling_utils.get_output_dim(txt_encoder)\n",
    "        self.txt_out = nn.Linear(txt_encoder_h, hidden_size)\n",
    "\n",
    "        cls_encoder_h = cat.modelling_utils.get_output_dim(cls_encoder)\n",
    "        self.cls_out = nn.Linear(cls_encoder_h, hidden_size)\n",
    "\n",
    "    def forward(self, text_input, labels_input):\n",
    "        \"\"\"\n",
    "        Compute logits for input (input_dict,) corresponding to the classes (classes_dict)\n",
    "\n",
    "        Optionally, you can provide additional keys in either input_dict or classes_dict\n",
    "        Specifically, attention_mask, head_mask and inputs_embeds\n",
    "        Howerver, one should not provide output_attentions and output_hidden_states\n",
    "\n",
    "        Args:\n",
    "            text_input: dict with key input_ids\n",
    "                input_ids: LongTensor[batch_size, text_seq_len], input to the text network\n",
    "            labels_input: dict with key input_ids\n",
    "                input_ids: LongTensor[n_classes, class_seq_len], a list of possible classes, each class described via text\n",
    "        \"\"\"\n",
    "        text_input, labels_input = cat.modelling_utils.maybe_format_inputs(text_input, labels_input)\n",
    "        cat.modelling_utils.validate_inputs(text_input, labels_input)\n",
    "\n",
    "        h_x = self.txt_encoder(**text_input)  # some tuple\n",
    "        h_x = h_x[0]  # FloatTensor[bs, text_seq_len, hidden]\n",
    "        h_x = h_x[:, 0]  # get CLS token representations, FloatTensor[bs, hidden]\n",
    "\n",
    "        h_c = self.cls_encoder(**labels_input)  # some tuple\n",
    "        h_c = h_c[0]  # FloatTensor[n_classes, class_seq_len, hidden]\n",
    "\n",
    "        h_c, _ = torch.max(h_c, dim=1)  # [n_classes, hidden]\n",
    "\n",
    "        # attention map\n",
    "        h_x = self.txt_out(h_x)\n",
    "        h_c = self.cls_out(h_c)\n",
    "\n",
    "        # make all class embeddings to have the same Euclidean norm\n",
    "        h_c = cat.modelling_utils.normalize_embeds(h_c)\n",
    "\n",
    "        # the scaling is extremely important\n",
    "        scaling = h_c.size(-1) ** 0.5\n",
    "        logits = (h_x @ h_c.T) / scaling  # [bs, n_classes]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the initial model distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1672, 0.1691, 0.1653, 0.1639, 0.1682, 0.1663],\n",
       "        [0.1647, 0.1660, 0.1667, 0.1660, 0.1698, 0.1668],\n",
       "        [0.1647, 0.1675, 0.1683, 0.1638, 0.1684, 0.1673]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_text_encoder = transformers.BertModel(\n",
    "    transformers.BertConfig(num_hidden_layers=2, intermediate_size=256)\n",
    ")\n",
    "random_label_encoder = transformers.BertModel(\n",
    "    transformers.BertConfig(num_hidden_layers=2, intermediate_size=256)\n",
    ")\n",
    "random_model = cat.ClassAttentionModel(\n",
    "    random_text_encoder, random_label_encoder, hidden_size=768\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 100, size=[3, 5])\n",
    "c = torch.unique(torch.randint(0, 50, size=[7, 1])).unsqueeze(1)\n",
    "\n",
    "logits = random_model(x, c)\n",
    "p = F.softmax(logits, -1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, same thing but with the DistilBERT initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertTokenizerFast' object has no attribute 'encode_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7ac316d11fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertTokenizerFast' object has no attribute 'encode_batch'"
     ]
    }
   ],
   "source": [
    "MODEL = 'distilbert-base-uncased'\n",
    "_tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "_text_encoder = transformers.AutoModel.from_pretrained(MODEL)\n",
    "_label_encoder = transformers.AutoModel.from_pretrained(MODEL)\n",
    "\n",
    "_model = cat.ClassAttentionModel(\n",
    "    _text_encoder, _label_encoder, hidden_size=768\n",
    ")\n",
    "\n",
    "x = _tokenizer.encode_plus([\n",
    "    \"Loads dataset with zero-shot classes, creates collators and dataloaders\",\n",
    "    \"\"\n",
    "])\n",
    "c = torch.unique(torch.randint(0, 50, size=[7, 1])).unsqueeze(1)\n",
    "\n",
    "logits = random_model(x, c)\n",
    "p = F.softmax(logits, -1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cat",
   "language": "python",
   "name": "cat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
