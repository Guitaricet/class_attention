{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Attetion with BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_dim(model):\n",
    "    config = model.config\n",
    "    if isinstance(config, transformers.DistilBertConfig):\n",
    "        return config.hidden_dim\n",
    "    \n",
    "    return config.hidden_size\n",
    "\n",
    "\n",
    "class ClassAttentionModel(nn.Module):\n",
    "    def __init__(self, txt_encoder, cls_encoder, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.txt_encoder = txt_encoder\n",
    "        self.cls_encoder = cls_encoder\n",
    "        \n",
    "        txt_encoder_h = get_output_dim(txt_encoder)\n",
    "        self.txt_out = nn.Sequential(nn.Linear(txt_encoder_h, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        cls_encoder_h = get_output_dim(cls_encoder)\n",
    "        self.cls_out = nn.Linear(cls_encoder_h, hidden_size)\n",
    "    \n",
    "    def forward(self, input_dict, classes_dict):\n",
    "        \"\"\"\n",
    "        Compute logits for input (input_dict,) corresponding to the classes (classes_dict)\n",
    "\n",
    "        Optionally, you can provide additional keys in either input_dict or classes_dict\n",
    "        Specifically, attention_mask, head_mask and inputs_embeds\n",
    "        Howerver, you cannot provide output_attentions and output_hidden_states\n",
    "\n",
    "        Args:\n",
    "            input_ids: dict with key input_ids\n",
    "                input_ids: LongTensor[batch_size, text_seq_len], input to the text network\n",
    "            classes_ids: dict with key input_ids\n",
    "                input_ids: LongTensor[n_classes, class_seq_len], a list of possible classes, each class described via text\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        h_x = self.txt_encoder(**input_dict)  # some tuple\n",
    "        h_x = h_x[0]  # FloatTensor[bs, text_seq_len, hidden]\n",
    "        h_x = h_x[:, 0]  # get CLS token representations, FloatTensor[bs, hidden]\n",
    "\n",
    "        h_c = self.cls_encoder(**classes_dict)  # some tuple\n",
    "        h_c = h_c[0]  # FloatTensor[n_classes, class_seq_len, hidden]\n",
    "\n",
    "        h_c, _ = torch.max(h_c, dim=1)  # [n_classes, hidden]\n",
    "        \n",
    "        # attention map\n",
    "        h_x = self.txt_out(h_x)\n",
    "        h_c = self.cls_out(h_c)\n",
    "\n",
    "        logits = h_x @ h_c.T  # [bs, n_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset news_category (/Users/vladislavlialin/.cache/huggingface/datasets/news_category/default/0.0.0/e1ca79a7dd2ddfef8393f386829a339f4212bea93dface547d0de38cbecfa97b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY', 'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK', 'BUSINESS', 'COMEDY', 'SPORTS', 'BLACK VOICES', 'HOME & LIVING', 'PARENTS', 'THE WORLDPOST', 'WEDDINGS', 'WOMEN', 'IMPACT', 'DIVORCE', 'CRIME', 'MEDIA', 'WEIRD NEWS', 'GREEN', 'WORLDPOST', 'RELIGION', 'STYLE', 'SCIENCE', 'WORLD NEWS', 'TASTE', 'TECH', 'MONEY', 'ARTS', 'FIFTY', 'GOOD NEWS', 'ARTS & CULTURE', 'ENVIRONMENT', 'COLLEGE', 'LATINO VOICES', 'CULTURE & ARTS', 'EDUCATION']\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"Fraser/news-category-dataset\")\n",
    "all_classes = dataset['train'].features['category_num'].names\n",
    "print(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot class frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WORLDPOST', 'SPORTS', 'MEDIA', 'RELIGION', 'WEIRD NEWS'}\n",
      "original dataset len 160682\n",
      "Train dataset    len 148208\n"
     ]
    }
   ],
   "source": [
    "train_classes = random.sample(all_classes, int(len(all_classes) * 0.9))\n",
    "train_dataset = [d for d in dataset['train'] if d['category'] in train_classes]\n",
    "valid_classes = set(all_classes).difference(train_classes)\n",
    "print(valid_classes)\n",
    "print(f'original dataset len {len(dataset[\"train\"])}')\n",
    "print(f'Train dataset    len {len(train_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_classes(batch_classes, all_classes, n_extra_classes=None):\n",
    "    if n_extra_classes is None:\n",
    "        n_extra_classes = random.randint(0, len(all_classes) - 1)  # TODO: experiment with distribution\n",
    "\n",
    "    extra_classes = random.sample(all_classes, n_extra_classes)  # TODO: experiment with distribution\n",
    "\n",
    "    new_classes = list(set(batch_classes) | set(extra_classes))\n",
    "    return new_classes\n",
    "\n",
    "\n",
    "def get_class_ids(batch_classes, possible_classes):\n",
    "    return torch.LongTensor([possible_classes.index(c) for c in batch_classes])\n",
    "\n",
    "\n",
    "_batch_classes = ['ARTS', 'BUSINESS', 'TRAVEL', 'POLITICS', 'ENTERTAINMENT', 'HEALTHY LIVING', 'ENTERTAINMENT']\n",
    "_possible_classes = ['ENTERTAINMENT', 'BUSINESS', 'HEALTHY LIVING', 'TRAVEL', 'HOME & LIVING', 'POLITICS', 'TECH', 'WOMEN', 'ARTS', 'GREEN']\n",
    "_expected_class_ids = torch.LongTensor([8, 1, 3, 5, 0, 2, 0])\n",
    "assert torch.equal(get_class_ids(_batch_classes, _possible_classes), _expected_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_MODEL = 'bert-base-uncased'\n",
    "CLS_MODEL = TXT_MODEL\n",
    "\n",
    "txt_tokenizer = AutoTokenizer.from_pretrained(TXT_MODEL)\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(CLS_MODEL)\n",
    "\n",
    "txt_encoder = AutoModel.from_pretrained(TXT_MODEL)\n",
    "cls_encoder = AutoModel.from_pretrained(CLS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_without_txt_encoder_embeddings(model):\n",
    "    return (p for n, p in model.named_parameters() if 'cls_encoder.embeddings' not in n)\n",
    "\n",
    "\n",
    "model = ClassAttentionModel(txt_encoder, cls_encoder, hidden_size=128)\n",
    "assert len(list(model.named_parameters())) - 5 == len(list(get_parameters_without_txt_encoder_embeddings(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size=32, device=None):\n",
    "    device = device or model.device\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "\n",
    "    n_matched = 0\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        text = batch['headline']\n",
    "\n",
    "        batch_classes = batch['category']\n",
    "        possible_classes = add_extra_classes(batch_classes, all_classes)\n",
    "        labels = get_class_ids(batch_classes, possible_classes)\n",
    "\n",
    "        if device != 'cpu':\n",
    "            txt_input = {k: v.to(device) for k, v in txt_input.items()}\n",
    "            cls_input = {k: v.to(device) for k, v in cls_input.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        txt_input = txt_tokenizer(text, return_tensors='pt', padding=True)\n",
    "        cls_input = cls_tokenizer(possible_classes, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "\n",
    "        assert False\n",
    "        logits = model(txt_input, cls_input)\n",
    "\n",
    "        _, preds = torch.max(logits, -1)\n",
    "        n_matched += torch.sum(preds == labels)\n",
    "        \n",
    "    model.train()\n",
    "        \n",
    "    return {'accuracy': n_matched.detach().cpu().numpy() / len(dataset)}\n",
    "\n",
    "\n",
    "# TODO: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig val len         : 10043\n",
      "Train classes val len: 9275\n",
      "Valid classes val len: 768\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = [d for d in dataset['validation'] if d['category'] in train_classes]\n",
    "valid_zero_shot_dataset = [d for d in dataset['validation'] if d['category'] in valid_classes]\n",
    "\n",
    "print(f'Orig val len         : {len(dataset[\"validation\"])}')\n",
    "print(f'Train classes val len: {len(valid_dataset)}')\n",
    "print(f'Valid classes val len: {len(valid_zero_shot_dataset)}')\n",
    "\n",
    "assert len(dataset[\"validation\"]) == len(valid_dataset) + len(valid_zero_shot_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset[:batch_size], batch_size=batch_size)\n",
    "model = ClassAttentionModel(txt_encoder, cls_encoder, hidden_size=32)\n",
    "model = model.to(device)\n",
    "\n",
    "# do not update cls_encoder embeddings so that the embedding geometry is not corrupted\n",
    "parameters = get_parameters_without_txt_encoder_embeddings(model)\n",
    "\n",
    "# parameters = chain(model.txt_encoder.parameters(), model.txt_out.parameters(), model.cls_out.parameters())\n",
    "parameters = model.txt_out.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3f8ycps9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12107<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/vladislavlialin/Documents/class_atteniton/notebooks/wandb/run-20201201_115715-3f8ycps9/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/vladislavlialin/Documents/class_atteniton/notebooks/wandb/run-20201201_115715-3f8ycps9/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">sage-disco-48</strong>: <a href=\"https://wandb.ai/guitaricet/uncategorized/runs/3f8ycps9\" target=\"_blank\">https://wandb.ai/guitaricet/uncategorized/runs/3f8ycps9</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3f8ycps9). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sweet-yogurt-49</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/guitaricet/uncategorized\" target=\"_blank\">https://wandb.ai/guitaricet/uncategorized</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/guitaricet/uncategorized/runs/1cp5kv50\" target=\"_blank\">https://wandb.ai/guitaricet/uncategorized/runs/1cp5kv50</a><br/>\n",
       "                Run data is saved locally in <code>/Users/vladislavlialin/Documents/class_atteniton/notebooks/wandb/run-20201201_115743-1cp5kv50</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19ff89c53dc4a7b82580b005a6bbce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=1000.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a5fabcf8134e028ffb4e4024626bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-74689d10ef39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb.watch(model)\n",
    "\n",
    "epochs = 1000\n",
    "eval_freq = 1000\n",
    "\n",
    "all_classes = dataset['train'].features['category_num'].names\n",
    "\n",
    "step = 0\n",
    "\n",
    "for e in tqdm(range(epochs), desc='epochs'):\n",
    "    for batch in tqdm(dataloader):\n",
    "        step += 1\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        text = batch['headline']\n",
    "\n",
    "        batch_classes = batch['category']\n",
    "\n",
    "#         possible_classes = add_extra_classes(batch_classes, all_classes)\n",
    "\n",
    "        possible_classes = batch_classes\n",
    "        labels = get_class_ids(batch_classes, possible_classes)\n",
    "\n",
    "        txt_input = txt_tokenizer(text, return_tensors='pt', padding=True)\n",
    "        cls_input = cls_tokenizer(possible_classes, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "\n",
    "        if device != 'cpu':\n",
    "            txt_input = {k: v.to(device) for k, v in txt_input.items()}\n",
    "            cls_input = {k: v.to(device) for k, v in cls_input.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        assert False\n",
    "\n",
    "        logits = model(txt_input, cls_input)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)        \n",
    "        _, preds = logits.max(-1)\n",
    "\n",
    "        wandb.log({'loss': loss, 'train/batch_accuracy': sum(preds == labels) / batch_size})\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % eval_freq != 0:\n",
    "            continue\n",
    "\n",
    "        eval_train_classes = evaluate(model, valid_dataset[:100])\n",
    "        eval_valid_classes = evaluate(model, valid_zero_shot_dataset[:100])\n",
    "\n",
    "        wandb.log({'valid/train_classes/accuracy': eval_train_classes['accuracy'],\n",
    "                   'valid/valid_classes/accuracy': eval_valid_classes['accuracy']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1005,  1055, 20554,  1005, 12934,  2015,  5035, 18528,  4895,\n",
       "         11256,  1999,  3147,  2330,   102],\n",
       "        [  101,  1996,  5264, 27857,  3527,  1999,  6613,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [  101,  1020,  3971,  1037,  1002,  2260,  6263, 11897,  2052,  2393,\n",
       "          1996,  4610,   102,     0,     0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4038],\n",
       "        [2840],\n",
       "        [2449]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COMEDY', 'ARTS', 'BUSINESS']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COMEDY', 'ARTS', 'BUSINESS']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on train classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = evaluate(model, valid_dataset, batch_size=32)\n",
    "\n",
    "print(f'Accuracy: {logs['accuracy']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on test classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = evaluate(model, valid_zero_shot_dataset, batch_size=32)\n",
    "\n",
    "print(f'Accuracy: {logs['accuracy']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
